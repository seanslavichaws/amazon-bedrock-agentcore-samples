{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock AgentCore Observability with LlamaIndex hosted outside of AgentCore Runtime\n",
    "\n",
    "This notebook demonstrates how to use setup observability for a [Llama Index Agent](https://docs.llamaindex.ai/en/stable/use_cases/agents/)  agent hosted outside of Amazon Bedrock AgentCore Runtime. Once you have completed the setup, you will be able to view the internal decision making process of the LlamaIndex agent in GenAI Observability dashboard in Amazon CloudWatch.\n",
    "\n",
    "## What you'll learn\n",
    "- How to set up LlamaIndex agent with Amazon OpenTelemetry Python Instrumentation\n",
    "- How to visualize and analyze agent traces in Amazon CloudWatch GenAI Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "- Enable transaction search on Amazon CloudWatch. First-time users must enable CloudWatch Transaction Search to view Bedrock AgentCore spans and traces. To enable transaction search, please refer to the our [documentation](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Enable-TransactionSearch.html).\n",
    "- Log group and Log stream configured on Amazon Cloudwatch to be added to the environment variables.\n",
    "- AWS account with Amazon Bedrock Model access to Claude Haiku with Model ID: anthropic.claude-3-haiku-20240307-v1:0\n",
    "- AWS credentials configured using `aws configure` \n",
    "- .env file updated with environment variables variables. An example is provided in `.env.example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies. Please check that `aws-opentelemetry-distro` in your requirements.txt file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying pre-requisites\n",
    "\n",
    "Before starting let's create a log group and a log stream for AgentCore observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "cloudwatch_client = boto3.client(\"logs\")\n",
    "response = cloudwatch_client.create_log_group(\n",
    "    logGroupName='agents/llama-index-agent-logs'\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cloudwatch_client.create_log_stream(\n",
    "    logGroupName='agents/llama-index-agent-logs',\n",
    "    logStreamName='default'\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling transactional search\n",
    "\n",
    "To run this example you first need to enable transactional search. You can do so in the AWS console following this [link](https://console.aws.amazon.com/cloudwatch/home#xray:settings/transaction-search).\n",
    "\n",
    "Once in this page, click on edit and set the option to ingest spans as structured logs in the OpenTelemetry format\n",
    "\n",
    "![image.png](./images/transactional_search.png)\n",
    "![image.png](./images/transactional_search2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "To enable observability for your LlamaIndex agent and send telemetry data to Amazon CloudWatch, you'll need to configure the following environment variables. We use a `.env` file to manage these settings securely, keeping sensitive AWS credentials separate from your code while making it easy to switch between different environments.\n",
    "\n",
    "**Ensure your AWS credentials are configured**\n",
    "\n",
    "We will create a `.env` file for configuring the environment variables. Use `env.example` as a template.\n",
    "\n",
    "Required Environment Variables:\n",
    "\n",
    "| Variable | Value | Purpose |\n",
    "|----------|-------|---------|\n",
    "| `OTEL_PYTHON_DISTRO` | `aws_distro` | Use AWS Distro for OpenTelemetry (ADOT) |\n",
    "| `OTEL_PYTHON_CONFIGURATOR` | `aws_configurator` | Set AWS configurator for ADOT SDK |\n",
    "| `OTEL_EXPORTER_OTLP_PROTOCOL` | `http/protobuf` | Configure export protocol |\n",
    "| `OTEL_EXPORTER_OTLP_LOGS_HEADERS` | `x-aws-log-group=<YOUR-LOG-GROUP>,x-aws-log-stream=<YOUR-LOG-STREAM>,x-aws-metric-namespace=<YOUR-NAMESPACE>` | Direct logs to CloudWatch groups |\n",
    "| `OTEL_RESOURCE_ATTRIBUTES` | `service.name=<YOUR-AGENT-NAME>` | Identify your agent in observability data |\n",
    "| `AGENT_OBSERVABILITY_ENABLED` | `true` | Activate ADOT pipeline |\n",
    "| `AWS_REGION` | `<YOUR-REGION>` | AWS Region |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "# AWS OpenTelemetry Distribution\n",
    "OTEL_PYTHON_DISTRO=aws_distro\n",
    "OTEL_PYTHON_CONFIGURATOR=aws_configurator\n",
    "\n",
    "# Export Protocol\n",
    "OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n",
    "OTEL_TRACES_EXPORTER=otlp\n",
    "\n",
    "# CloudWatch Integration (uncomment and configure as needed)\n",
    "OTEL_EXPORTER_OTLP_LOGS_HEADERS=x-aws-log-group=agents/llama-index-agent-logs,x-aws-log-stream=default,x-aws-metric-namespace=bedrock-agentcore\n",
    "\n",
    "# Service Identification\n",
    "OTEL_RESOURCE_ATTRIBUTES=service.name=agentic-llamaindex-agentcore\n",
    "# Enable Agent Observability\n",
    "AGENT_OBSERVABILITY_ENABLED=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Environment Variables\n",
    "\n",
    "Let's load the environment variables from the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Display the OTEL-related environment variables\n",
    "otel_vars = [\n",
    "    \"OTEL_PYTHON_DISTRO\",\n",
    "    \"OTEL_PYTHON_CONFIGURATOR\",\n",
    "    \"OTEL_EXPORTER_OTLP_PROTOCOL\",\n",
    "    \"OTEL_EXPORTER_OTLP_LOGS_HEADERS\",\n",
    "    \"OTEL_RESOURCE_ATTRIBUTES\",\n",
    "    \"AGENT_OBSERVABILITY_ENABLED\",\n",
    "    \"OTEL_TRACES_EXPORTER\"\n",
    "]\n",
    "\n",
    "print(\"OpenTelemetry Configuration:\")\n",
    "for var in otel_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        print(f\"{var}={value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a LlamaIndex Agent in a python file\n",
    "\n",
    "LlamaIndex arithmetic agent implementation is provided in `llama_index_agent.py`. It is a simple arithmetic agent set up with a Claude 3 Haiku model from Amazon Bedrock. The AWS OpenTelemetry distro will automatically handle tracer provider setup when using `opentelemetry-instrument` command.\n",
    "\n",
    "The agent is a simple arithmetic agent that:\n",
    "\n",
    "- Creates a FunctionAgent using AWS Bedrock's Claude Haiku model\n",
    "- Defines basic arithmetic tools for addition and multiplication\n",
    "- Gives the agent a task to calculate a simple mathematical expression: (121 + 2) * 5\n",
    "- Runs the agent and returns the calculated result\n",
    "\n",
    "The Agent is Configured with the following:\n",
    "\n",
    "- Two arithmetic function tools: add and multiply\n",
    "- Amazon Bedrock's Claude Haiku model as its Large Language Model\n",
    "- OpenTelemetry instrumentation for tracing and observability\n",
    "\n",
    "The agent is executed asynchronously using the agent's run method, which processes the math query and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_index_agent.py\n",
    "###########################\n",
    "#### Agent Code below: ####\n",
    "###########################\n",
    "import os\n",
    "import asyncio\n",
    "import logging\n",
    "from llama_index.observability.otel import LlamaIndexOpenTelemetry\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "# Initialize OpenTelemetry instrumentation for LlamaIndex\n",
    "instrumentor = LlamaIndexOpenTelemetry(debug=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure LlamaIndex logging\n",
    "logging.getLogger(\"llamaindex\").setLevel(logging.INFO)\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def get_bedrock_model():\n",
    "    model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-haiku-20240307-v1:0\")\n",
    "    region = os.getenv(\"AWS_DEFAULT_REGION\", \"us-west-2\")\n",
    "    \n",
    "    try:\n",
    "        # Let boto3 handle credential resolution automatically\n",
    "        bedrock_model = BedrockConverse(\n",
    "            model=model_id,\n",
    "            region_name=region,\n",
    "            # No explicit credentials - boto3 will find them automatically\n",
    "        )\n",
    "        logger.info(f\"Successfully initialized Bedrock model: {model_id} in region: {region}\")\n",
    "        return bedrock_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Bedrock model: {str(e)}\")\n",
    "        logger.error(\"Please ensure you have proper AWS credentials configured and access to the Bedrock model\")\n",
    "        raise\n",
    "\n",
    "# Initialize the model\n",
    "bedrock_model = get_bedrock_model()\n",
    "\n",
    "# Create the arithmetic agent\n",
    "agent = FunctionAgent(\n",
    "    tools=[add, multiply],\n",
    "    llm=bedrock_model,\n",
    ")\n",
    "\n",
    "# Start listening\n",
    "instrumentor.start_registering()\n",
    "\n",
    "# Execute the arithmetic task\n",
    "query = \"\"\"What is (121 + 2) * 5?\"\"\"\n",
    "\n",
    "async def main():\n",
    "    result = await agent.run(query)\n",
    "    print(\"Result:\", str(result))\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AWS OpenTelemetry Python Distro\n",
    "\n",
    "Now that your environment is configured, let's understand how the observability happens. The [AWS OpenTelemetry Python Distro](https://pypi.org/project/aws-opentelemetry-distro/) automatically instruments your Strands agent to capture telemetry data without requiring code changes.\n",
    "\n",
    "This distribution provides:\n",
    "- **Auto-instrumentation** for your Strands Agent hosted outside of AgentCore Runtime (i.e. EC2, Lambda etc..)\n",
    "- **AWS-optimized configuration** for seamless CloudWatch integration  \n",
    "\n",
    "### Running Your Instrumented Agent\n",
    "\n",
    "To capture traces from your LlamaIndex agent, use the `opentelemetry-instrument` command instead of running Python directly. This automatically applies instrumentation using the environment variables from your `.env` file:\n",
    "\n",
    "```bash\n",
    "opentelemetry-instrument python llama_index_agent.py\n",
    "```\n",
    "\n",
    "This command will:\n",
    "\n",
    "- Load your OTEL configuration from the .env file\n",
    "- Automatically instrument LlamaIndex, Amazon Bedrock calls, agent tool and databases, and other requests made by agent\n",
    "- Send traces to CloudWatch\n",
    "- Enable you to visualize the agent's decision-making process in the GenAI Observability dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!opentelemetry-instrument python llama_index_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adding Session Tracking\n",
    "\n",
    "To correlate traces across multiple agent runs, you can associate a session ID with your telemetry data using OpenTelemetry baggage:\n",
    "\n",
    "```python\n",
    "from opentelemetry import baggage, context\n",
    "ctx = baggage.set_baggage(\"session.id\", session_id)\n",
    "```\n",
    "\n",
    "Run the session-enabled version:\n",
    "```bash\n",
    "opentelemetry-instrument python llama_indedx_agent_with_session.py --session-id \"user-session-123\"\n",
    "```\n",
    "\n",
    "## 7. Custom Metadata for Analysis\n",
    "Add custom attributes to enable filtering, offline evaluations, and performance analysis. You would need to modify your agent code to accept additional parameters:\n",
    "```python\n",
    "ctx = baggage.set_baggage(\"user.type\", \"premium\")\n",
    "ctx = baggage.set_baggage(\"experiment.id\", \"llama-agent\")\n",
    "ctx = baggage.set_baggage(\"conversation.topic\", \"arithmetic\")\n",
    "```\n",
    "\n",
    "Example commands with custom metadata:\n",
    "\n",
    "```bash\n",
    "# A/B testing different experiments\n",
    "opentelemetry-instrument python agent.py --session-id \"session-123\" --experiment-id \"model-a\"\n",
    "opentelemetry-instrument python agent.py --session-id \"session-124\" --experiment-id \"model-b\"\n",
    "\n",
    "# Tracking different user types\n",
    "opentelemetry-instrument python agent.py --session-id \"session-125\" --user-type \"premium\"\n",
    "opentelemetry-instrument python agent.py --session-id \"session-126\" --user-type \"free\"\n",
    "\n",
    "# Offline evaluation runs\n",
    "opentelemetry-instrument python agent.py --session-id \"eval-001\" --dataset \"golden-set-v1\"\n",
    "```\n",
    "These attributes appear in CloudWatch traces for advanced filtering and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama_index_agent_with_session.py\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import asyncio\n",
    "from opentelemetry import baggage, context\n",
    "from llama_index.observability.otel import LlamaIndexOpenTelemetry\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description='LlamaIndex Arithmetic Agent with Session Tracking')\n",
    "    parser.add_argument('--session-id', \n",
    "                       type=str, \n",
    "                       required=True,\n",
    "                       help='Session ID to associate with this agent run')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def set_session_context(session_id):\n",
    "    \"\"\"Set the session ID in OpenTelemetry baggage for trace correlation\"\"\"\n",
    "    ctx = baggage.set_baggage(\"session.id\", session_id)\n",
    "    token = context.attach(ctx)\n",
    "    logging.info(f\"Session ID '{session_id}' attached to telemetry context\")\n",
    "    return token\n",
    "\n",
    "###########################\n",
    "#### Agent Code below: ####\n",
    "###########################\n",
    "\n",
    "# Initialize OpenTelemetry instrumentation for LlamaIndex\n",
    "instrumentor = LlamaIndexOpenTelemetry(debug=True)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure LlamaIndex logging\n",
    "logging.getLogger(\"llamaindex\").setLevel(logging.INFO)\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def get_bedrock_model():\n",
    "    model_id = os.getenv(\"BEDROCK_MODEL_ID\", \"anthropic.claude-3-haiku-20240307-v1:0\")\n",
    "    region = os.getenv(\"AWS_DEFAULT_REGION\", \"us-west-2\")\n",
    "\n",
    "    try:\n",
    "        # Let boto3 handle credential resolution automatically\n",
    "        bedrock_model = BedrockConverse(\n",
    "            model=model_id,\n",
    "            region_name=region,\n",
    "            # No explicit credentials - boto3 will find them automatically\n",
    "        )\n",
    "        logger.info(f\"Successfully initialized Bedrock model: {model_id} in region: {region}\")\n",
    "        return bedrock_model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Bedrock model: {str(e)}\")\n",
    "        logger.error(\"Please ensure you have proper AWS credentials configured and access to the Bedrock model\")\n",
    "        raise\n",
    "\n",
    "async def run_agent(query):\n",
    "    # Initialize the model\n",
    "    bedrock_model = get_bedrock_model()\n",
    "\n",
    "    # Create the arithmetic agent\n",
    "    agent = FunctionAgent(\n",
    "        tools=[add, multiply],\n",
    "        llm=bedrock_model,\n",
    "    )\n",
    "\n",
    "    # Start listening\n",
    "    instrumentor.start_registering()\n",
    "\n",
    "    # Execute the arithmetic task\n",
    "    result = await agent.run(query)\n",
    "    print(\"Result:\", str(result))\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    args = parse_arguments()\n",
    "\n",
    "    # Set session context for telemetry\n",
    "    context_token = set_session_context(args.session_id)\n",
    "\n",
    "    try:\n",
    "        # Execute the arithmetic task\n",
    "        query = \"\"\"What is (121 + 2) * 5?\"\"\"\n",
    "\n",
    "        # Run the async function in the event loop\n",
    "        result = asyncio.run(run_agent(query))\n",
    "\n",
    "    finally:\n",
    "        # Detach context when done\n",
    "        try:\n",
    "            context.detach(context_token)\n",
    "            logger.info(f\"Session context for '{args.session_id}' detached\")\n",
    "        except ValueError as e:\n",
    "            # Handle the context detachment error that might occur\n",
    "            logger.error(f\"Error detaching context: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!opentelemetry-instrument python llama_index_agent_with_session.py --session-id \"session-1234\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gen AI Observability Dashboard Understanding the Traces in AWS CloudWatch\n",
    "\n",
    "Once your LlamaIndex agent runs with OpenTelemetry instrumentation, you can visualize and analyze the traces in AWS CloudWatch's GenAI Observability dashboard. Navigate to Bedrock Agentcore and click on the Agent you just created.\n",
    "\n",
    "#### Sessions View Page:\n",
    "\n",
    "![llama_index_sessions.png](images/llama_index_sessions.png)\n",
    "\n",
    "\n",
    "#### Trace View Page:\n",
    "Trace View:\n",
    "\n",
    "![llama_index_sessions.png](images/llama_index_traces.png)\n",
    "\n",
    "\n",
    "Trace details:\n",
    "\n",
    "![llama_index_sessions.png](images/llama_index_trace_details.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Troubleshooting\n",
    "\n",
    "If you're not seeing traces in Amazon CloudWatch or X-Ray, check the following:\n",
    "\n",
    "1. **AWS Credentials**: Ensure your AWS credentials are properly configured\n",
    "2. **IAM Permissions**: Make sure your IAM user/role has permissions for CloudWatch\n",
    "3. **Region**: Confirm you're looking in the correct AWS region\n",
    "4. **Environment Variables**: Verify all OTEL_* environment variables are set correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion \n",
    "\n",
    "Congratulations you implemented and instrumented a LlamaIndex Agent with Amazon Bedrock Model which has observability through Amazon CloudWatch.\n",
    "\n",
    "- LlamaIndex arithmetic agent.\n",
    "- Full OpenTelemetry tracing\n",
    "- Traces for Amazon Bedrock calls, LlamaIndex operations, etc.\n",
    "- Service name: agentic-llamaindex-agentcore "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps\n",
    "\n",
    "Now that you have LlamaIndex with OpenTelemetry set up, you can:\n",
    "\n",
    "1. **Add More Agents**: Create a multi-agent architectures with different patterns\n",
    "2. **Add Tools to your agent**: Integrate search tools, API tools, or custom tools\n",
    "3. **[Set Up Alarms](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)**: Create alarms on the metrics that are important to your business like `latency`, `token input`, and `token output` etc..\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nb_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
